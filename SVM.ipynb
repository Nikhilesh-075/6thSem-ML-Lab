{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbthfu101tq/hRuLVh3OpS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhilesh-075/6thSem-ML-Lab/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYj2uuCs1Bvy",
        "outputId": "9e354d4a-5948-459d-d881-ebcca27bcd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load the dataset (using Iris dataset as an example)\n",
        "def load_iris_data():\n",
        "    from sklearn.datasets import load_iris\n",
        "    data = load_iris()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "    return X, y\n",
        "\n",
        "# 2. Linear kernel for the SVM (dot product)\n",
        "def linear_kernel(x1, x2):\n",
        "    return np.dot(x1, x2)\n",
        "\n",
        "# 3. Compute the SVM decision function (weight * x + bias)\n",
        "def decision_function(X, w, b):\n",
        "    return np.dot(X, w) + b\n",
        "\n",
        "# 4. Compute the hinge loss function for SVM\n",
        "def hinge_loss(X, y, w, b, C):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        margin = y[i] * (np.dot(X[i], w) + b)\n",
        "        loss += max(0, 1 - margin)\n",
        "    return 0.5 * np.dot(w, w) + C * loss  # Regularization term + hinge loss\n",
        "\n",
        "# 5. Compute the gradient for the SVM\n",
        "def compute_gradient(X, y, w, b, C):\n",
        "    dw = np.zeros_like(w)\n",
        "    db = 0\n",
        "    for i in range(len(X)):\n",
        "        if y[i] * (np.dot(X[i], w) + b) < 1:\n",
        "            dw -= C * y[i] * X[i]\n",
        "            db -= C * y[i]\n",
        "        else:\n",
        "            dw += w  # Regularization term\n",
        "    return dw, db\n",
        "\n",
        "# 6. Train the SVM using Stochastic Gradient Descent (SGD)\n",
        "def train_svm(X, y, C=1, learning_rate=0.01, epochs=1000):\n",
        "    # Initialize weights and bias\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    n = len(X)\n",
        "\n",
        "    # SGD loop\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(n):\n",
        "            xi = X[i]\n",
        "            yi = y[i]\n",
        "            if yi * (np.dot(xi, w) + b) < 1:  # Misclassified point\n",
        "                dw = w - C * yi * xi\n",
        "                db = -C * yi\n",
        "            else:  # Correctly classified point\n",
        "                dw = w\n",
        "\n",
        "            # Update the weights and bias\n",
        "            w -= learning_rate * dw\n",
        "            b -= learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# 7. Predict the labels for the test data\n",
        "def predict_svm(X, w, b):\n",
        "    y_pred = np.sign(np.dot(X, w) + b)\n",
        "    return y_pred\n",
        "\n",
        "# 8. Evaluate the model's accuracy\n",
        "def evaluate_model(X_train, y_train, X_test, y_test, C=1, learning_rate=0.01, epochs=1000):\n",
        "    w, b = train_svm(X_train, y_train, C, learning_rate, epochs)\n",
        "    y_pred = predict_svm(X_test, w, b)\n",
        "    accuracy = np.mean(y_pred == y_test)\n",
        "    return accuracy\n",
        "\n",
        "# 9. Split the data into training and testing sets\n",
        "def train_test_split(X, y, test_size=0.2):\n",
        "    # Calculate the number of test samples\n",
        "    num_test_samples = int(len(X) * test_size)\n",
        "\n",
        "    # Shuffle the dataset indices\n",
        "    indices = np.random.permutation(len(X))\n",
        "\n",
        "    # Split the data into train and test\n",
        "    X_train, X_test = X[indices[num_test_samples:]], X[indices[:num_test_samples]]\n",
        "    y_train, y_test = y[indices[num_test_samples:]], y[indices[:num_test_samples]]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# 10. Example of usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X, y = load_iris_data()\n",
        "\n",
        "    # Convert target labels to -1 and 1 (SVM requires binary classification)\n",
        "    # We will use only two classes for this example (class 0 and class 1).\n",
        "    X = X[y != 2]  # Only take class 0 and class 1\n",
        "    y = y[y != 2]\n",
        "    y = np.where(y == 0, -1, 1)  # Convert labels 0 to -1 and 1 remains 1\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = evaluate_model(X_train, y_train, X_test, y_test, C=1, learning_rate=0.01, epochs=1000)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ]
}